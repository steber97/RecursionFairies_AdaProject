{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "In this milestone, our goal has been the collection, the cleaning and the analysis of the data in the datasets that we have decided to use for our project.\n",
    "\n",
    "Given the high number of datasets that we have analyzed, we decided to perform the data cleaning process in separate files, in order to avoid one single notebook with a huge amount of lines of code, all independent from each other.\n",
    "\n",
    "The notebook that we have written in order to clean and analyze the datasets are:\n",
    "- crime.ipynb: a crime dataset is analyzed, we hope to extract good insights on the quality of life given the number of crimes committed in all states\n",
    "- McDonalds.ipynb: a dataset of alimentation quality (number of fast food restaurants per state, obesity rate, people eating unhealthy).\n",
    "- school_dataset.ipynb: a dataset with tuition fees of private schools and elementary schools. \n",
    "- homelessData.ipynb: We extracted homeless people figure from 2007 to 2018.\n",
    "- rentData.ipynb: We selected the rent price of 2018 for 4 types of rents: 1, 2, 3 and 4 bedrooms, and studios.\n",
    "- csv_datasets.ipynb: we analyze and extract features related to University rankings, life expectancy and personal income.\n",
    "\n",
    "Every notebook output the final DataFrame in a Pickle file, which are all imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put all dataframes in a list\n",
    "df_list = []\n",
    "\n",
    "for pck in os.listdir(\"Pickles/\"):\n",
    "    # We avoid all files which are not pickle files (like .ipynb_checkpoints)\n",
    "    if pck.endswith(\".pickle\"):\n",
    "        df_list.append(pd.read_pickle(\"Pickles/\" + pck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_list)-1):\n",
    "    \n",
    "    # We want to be sure that all DataFrames have the column \"State\"\n",
    "    if \"State\" in df_list[i].columns and \"State\" in df_list[i+1].columns:\n",
    "        print(\"Correct\")\n",
    "    else:\n",
    "        print(\"Incorrect\")\n",
    "    # we merge the left dataframe (position i) into the right dataset (i+1)\n",
    "    df_list[i+1] = df_list[i].merge(df_list[i+1], left_on=\"State\", right_on='State')\n",
    "    \n",
    "# At the end, the last dataframe in the list is the final merged DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index_x', 'State', 'Population_x', 'Violent_Crime',\n",
       "       'Murder_and_Manslaughter', 'Rape', 'Robbery', 'Aggravated_Assoult',\n",
       "       'Property_crime', 'Burglary', 'Theft', 'Motor_Vehicle_theft',\n",
       "       'Price_2018_Studio', 'Price_2018_1br', 'Price_2018_2br',\n",
       "       'Price_2018_3br', 'Price_2018_4br', 'index_y', 'mc_donalds_per_100k',\n",
       "       'adult_obesity_rate', 'eating_vegetables_daily', 'median_income',\n",
       "       'Vegetable', 'Population_y', 'Per_capita_income', 'Life_Expectancy',\n",
       "       'N_of_colleges_universities', 'N_of_junior_colleges',\n",
       "       'N_of_technical_trade_schools', 'awards_per_value', 'exp_award_value',\n",
       "       'top_230_ranking_score', 'Overall Homeless, 2018',\n",
       "       'Overall Homeless, 2017', 'Overall Homeless, 2016',\n",
       "       'Overall Homeless, 2015', 'Overall Homeless, 2014',\n",
       "       'Overall Homeless, 2013', 'Overall Homeless, 2012',\n",
       "       'Overall Homeless, 2011', 'Overall Homeless, 2010',\n",
       "       'Overall Homeless, 2009', 'Overall Homeless, 2008',\n",
       "       'Overall Homeless, 2007', 'diabetes_prevalence', 'alcohol_prevalence',\n",
       "       'mean_physical_activity', 'mean_obesity', 'High_School_Fee',\n",
       "       'Elementary_School_Fee'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_list[-1]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that some columns are repeated: therefore we drop duplicate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Population</th>\n",
       "      <th>Violent_Crime</th>\n",
       "      <th>Murder_and_Manslaughter</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Aggravated_Assoult</th>\n",
       "      <th>Property_crime</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Theft</th>\n",
       "      <th>...</th>\n",
       "      <th>Overall Homeless, 2010</th>\n",
       "      <th>Overall Homeless, 2009</th>\n",
       "      <th>Overall Homeless, 2008</th>\n",
       "      <th>Overall Homeless, 2007</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>alcohol_prevalence</th>\n",
       "      <th>mean_physical_activity</th>\n",
       "      <th>mean_obesity</th>\n",
       "      <th>High_School_Fee</th>\n",
       "      <th>Elementary_School_Fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>4863300</td>\n",
       "      <td>532.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>96.4</td>\n",
       "      <td>388.2</td>\n",
       "      <td>2947.8</td>\n",
       "      <td>700.5</td>\n",
       "      <td>2006.3</td>\n",
       "      <td>...</td>\n",
       "      <td>6046</td>\n",
       "      <td>6080</td>\n",
       "      <td>5387</td>\n",
       "      <td>5452</td>\n",
       "      <td>16.19</td>\n",
       "      <td>43.6</td>\n",
       "      <td>49.5</td>\n",
       "      <td>39.75</td>\n",
       "      <td>7633</td>\n",
       "      <td>6388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>741894</td>\n",
       "      <td>804.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>141.9</td>\n",
       "      <td>114.6</td>\n",
       "      <td>540.6</td>\n",
       "      <td>3353.0</td>\n",
       "      <td>546.3</td>\n",
       "      <td>2394.7</td>\n",
       "      <td>...</td>\n",
       "      <td>1863</td>\n",
       "      <td>1992</td>\n",
       "      <td>1646</td>\n",
       "      <td>1642</td>\n",
       "      <td>11.89</td>\n",
       "      <td>59.5</td>\n",
       "      <td>59.8</td>\n",
       "      <td>35.70</td>\n",
       "      <td>6118</td>\n",
       "      <td>7544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>6931071</td>\n",
       "      <td>470.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>47.5</td>\n",
       "      <td>101.8</td>\n",
       "      <td>315.4</td>\n",
       "      <td>2978.4</td>\n",
       "      <td>544.4</td>\n",
       "      <td>2168.1</td>\n",
       "      <td>...</td>\n",
       "      <td>13711</td>\n",
       "      <td>14721</td>\n",
       "      <td>12488</td>\n",
       "      <td>14646</td>\n",
       "      <td>13.72</td>\n",
       "      <td>55.8</td>\n",
       "      <td>59.5</td>\n",
       "      <td>33.10</td>\n",
       "      <td>17339</td>\n",
       "      <td>6300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2988248</td>\n",
       "      <td>550.9</td>\n",
       "      <td>7.2</td>\n",
       "      <td>71.7</td>\n",
       "      <td>70.9</td>\n",
       "      <td>401.0</td>\n",
       "      <td>3268.6</td>\n",
       "      <td>795.5</td>\n",
       "      <td>2233.6</td>\n",
       "      <td>...</td>\n",
       "      <td>2762</td>\n",
       "      <td>2852</td>\n",
       "      <td>3255</td>\n",
       "      <td>3836</td>\n",
       "      <td>15.23</td>\n",
       "      <td>43.7</td>\n",
       "      <td>51.3</td>\n",
       "      <td>38.25</td>\n",
       "      <td>6580</td>\n",
       "      <td>4724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>California</td>\n",
       "      <td>39250017</td>\n",
       "      <td>445.3</td>\n",
       "      <td>4.9</td>\n",
       "      <td>34.9</td>\n",
       "      <td>139.6</td>\n",
       "      <td>265.9</td>\n",
       "      <td>2553.0</td>\n",
       "      <td>479.8</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>...</td>\n",
       "      <td>123480</td>\n",
       "      <td>123678</td>\n",
       "      <td>136531</td>\n",
       "      <td>138986</td>\n",
       "      <td>14.51</td>\n",
       "      <td>56.5</td>\n",
       "      <td>61.3</td>\n",
       "      <td>31.15</td>\n",
       "      <td>19235</td>\n",
       "      <td>11360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Population  Violent_Crime  Murder_and_Manslaughter   Rape  \\\n",
       "0     Alabama     4863300          532.3                      8.4   39.4   \n",
       "1      Alaska      741894          804.2                      7.0  141.9   \n",
       "2     Arizona     6931071          470.1                      5.5   47.5   \n",
       "3    Arkansas     2988248          550.9                      7.2   71.7   \n",
       "4  California    39250017          445.3                      4.9   34.9   \n",
       "\n",
       "   Robbery  Aggravated_Assoult  Property_crime  Burglary   Theft  ...  \\\n",
       "0     96.4               388.2          2947.8     700.5  2006.3  ...   \n",
       "1    114.6               540.6          3353.0     546.3  2394.7  ...   \n",
       "2    101.8               315.4          2978.4     544.4  2168.1  ...   \n",
       "3     70.9               401.0          3268.6     795.5  2233.6  ...   \n",
       "4    139.6               265.9          2553.0     479.8  1623.0  ...   \n",
       "\n",
       "   Overall Homeless, 2010  Overall Homeless, 2009  Overall Homeless, 2008  \\\n",
       "0                    6046                    6080                    5387   \n",
       "1                    1863                    1992                    1646   \n",
       "2                   13711                   14721                   12488   \n",
       "3                    2762                    2852                    3255   \n",
       "4                  123480                  123678                  136531   \n",
       "\n",
       "   Overall Homeless, 2007  diabetes_prevalence  alcohol_prevalence  \\\n",
       "0                    5452                16.19                43.6   \n",
       "1                    1642                11.89                59.5   \n",
       "2                   14646                13.72                55.8   \n",
       "3                    3836                15.23                43.7   \n",
       "4                  138986                14.51                56.5   \n",
       "\n",
       "   mean_physical_activity  mean_obesity  High_School_Fee  \\\n",
       "0                    49.5         39.75             7633   \n",
       "1                    59.8         35.70             6118   \n",
       "2                    59.5         33.10            17339   \n",
       "3                    51.3         38.25             6580   \n",
       "4                    61.3         31.15            19235   \n",
       "\n",
       "   Elementary_School_Fee  \n",
       "0                   6388  \n",
       "1                   7544  \n",
       "2                   6300  \n",
       "3                   4724  \n",
       "4                  11360  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the column Population_x into population, and drop index_x, index_y and Population_y\n",
    "df = df.rename(columns = {\n",
    "    \"Population_x\": \"Population\",\n",
    "})\n",
    "\n",
    "df = df.drop(['index_x', 'index_y', 'Population_y', 'median_income'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['State', 'Population', 'Violent_Crime', 'Murder_and_Manslaughter',\n",
       "       'Rape', 'Robbery', 'Aggravated_Assoult', 'Property_crime', 'Burglary',\n",
       "       'Theft', 'Motor_Vehicle_theft', 'Price_2018_Studio', 'Price_2018_1br',\n",
       "       'Price_2018_2br', 'Price_2018_3br', 'Price_2018_4br',\n",
       "       'mc_donalds_per_100k', 'adult_obesity_rate', 'eating_vegetables_daily',\n",
       "       'Vegetable', 'Per_capita_income', 'Life_Expectancy',\n",
       "       'N_of_colleges_universities', 'N_of_junior_colleges',\n",
       "       'N_of_technical_trade_schools', 'awards_per_value', 'exp_award_value',\n",
       "       'top_230_ranking_score', 'Overall Homeless, 2018',\n",
       "       'Overall Homeless, 2017', 'Overall Homeless, 2016',\n",
       "       'Overall Homeless, 2015', 'Overall Homeless, 2014',\n",
       "       'Overall Homeless, 2013', 'Overall Homeless, 2012',\n",
       "       'Overall Homeless, 2011', 'Overall Homeless, 2010',\n",
       "       'Overall Homeless, 2009', 'Overall Homeless, 2008',\n",
       "       'Overall Homeless, 2007', 'diabetes_prevalence', 'alcohol_prevalence',\n",
       "       'mean_physical_activity', 'mean_obesity', 'High_School_Fee',\n",
       "       'Elementary_School_Fee'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What comes next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all datasets cleaned and analyzed, we can further proceed in the realization of our project: firstly, we need to train a model which will enable us to predict the HDI index of every state of the USA, based on the dataset that we have created.\n",
    "\n",
    "This task is going to pose some challenges: in fact, our dataset has much more dimensions than samples and therefore there is going to be the risk of overfitting.\n",
    "\n",
    "We can try to counter this effect by performing some dimensionality reduction algorithms like PCA, Factor Analysis or Independent Component Analysis.\n",
    "\n",
    "As soon as we manage to reduce the dimension of our dataset, we can train the model using any regression technique like SVM or Ridge Regression.\n",
    "\n",
    "Once we compute the weights for our model, then we can build our index by adjusting the weights of our model to personalize it to the users' needs. We will offer a visual representation of it using sliders, so that the user can adjust the weights in an intuitive manner.\n",
    "\n",
    "Further, we may try to split our features into meaningful subsets (health, crime, education, monetary) in order to gauge their impact on our final HDI score. Moreover, we may try to investigate the relation among the above subsets and the actual features used to compute the UN HDI index (personal income, life expectancy and average years of education).\n",
    "\n",
    "To conclude, a further improvement could be trying to test the robustness of our model: it means that, whenever we drop any column (for instance if it happens that any of the organizations computing our metrics fails), we are still able to learn a good enough predictor for the HDI. The fact that our dataset has so many correlated features means that probably we can manage the loss of some columns, without losing much accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
