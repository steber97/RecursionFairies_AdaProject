{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "In this milestone, our goal has been the collection, the cleaning and the analysis of the data in the datasets that we have decided to use for our project.\n",
    "\n",
    "Given the high number of datasets that we have analyzed, we decided to perform the data cleaning process in separate files, in order to avoid one single notebook with a huge amount of lines of code, all independent from each other.\n",
    "\n",
    "The notebook that we have written in order to clean and analyze the datasets are:\n",
    "- crime.ipynb: a crime dataset is analyzed, we hope to extract good insights on the quality of life given the number of crimes committed in all states\n",
    "- McDonalds.ipynb: a dataset of alimentation quality (number of fast food restaurants per state, obesity rate, people eating unhealthy).\n",
    "- school_dataset.ipynb: a dataset with tuition fees of private schools and elementary schools. \n",
    "- homelessData.ipynb: We extracted homeless people figure from 2007 to 2018.\n",
    "- rentData.ipynb: We selected the rent price of 2018 for 4 types of rents: 1, 2, 3 and 4 bedrooms, and studios.\n",
    "- csv_datasets.ipynb: we analyze and extract features related to University rankings, life expectancy and personal income.\n",
    "- diabetes_physical_activity_alcohol_obesity.ipynb : We extract and analyse prevalence of physical activity, alcohol, diabetes and obesity.\n",
    "\n",
    "Every notebook output the final DataFrame in a Pickle file, which are all imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickles/rent.pickle\n",
      "The number of states is: 51\n",
      "Pickles/homeless.pickle\n",
      "The number of states is: 51\n",
      "Pickles/tuition_fees.pickle\n",
      "The number of states is: 51\n",
      "Pickles/mc_donalds.pickle\n",
      "The number of states is: 51\n",
      "Pickles/hv_data.pickle\n",
      "The number of states is: 51\n",
      "Pickles/crime.pickle\n",
      "The number of states is: 51\n",
      "Pickles/pop_inc_exp_edu.pickle\n",
      "The number of states is: 51\n"
     ]
    }
   ],
   "source": [
    "# We put all dataframes in a list\n",
    "df_list = []\n",
    "\n",
    "for pck in os.listdir(\"Pickles/\"):\n",
    "    # We avoid all files which are not pickle files (like .ipynb_checkpoints)\n",
    "    if pck.endswith(\".pickle\"):\n",
    "        print(\"Pickles/{}\".format(pck))\n",
    "        df_temp = pd.read_pickle(\"Pickles/\" + pck)\n",
    "        #Convert state names to lower case for future merges\n",
    "        df_temp[\"State\"] = df_temp[\"State\"].apply(lambda r: r.lower())\n",
    "        df_list.append(df_temp)\n",
    "        print(\"The number of states is: {}\".format(len(df_temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_list)-1):\n",
    "    \n",
    "    # We want to be sure that all DataFrames have the column \"State\"\n",
    "    if \"State\" in df_list[i].columns and \"State\" in df_list[i+1].columns:\n",
    "        print(\"Correct\")\n",
    "    else:\n",
    "        print(\"Incorrect\")\n",
    "    # we merge the left dataframe (position i) into the right dataset (i+1)\n",
    "    df_list[i+1] = df_list[i].merge(df_list[i+1], left_on=\"State\", right_on='State')\n",
    "    \n",
    "# At the end, the last dataframe in the list is the final merged DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['State', 'Price_2018_Studio', 'Price_2018_1br', 'Price_2018_2br',\n",
       "       'Price_2018_3br', 'Price_2018_4br', 'Overall Homeless, 2018',\n",
       "       'Overall Homeless, 2017', 'Overall Homeless, 2016',\n",
       "       'Overall Homeless, 2015', 'Overall Homeless, 2014',\n",
       "       'Overall Homeless, 2013', 'Overall Homeless, 2012',\n",
       "       'Overall Homeless, 2011', 'Overall Homeless, 2010',\n",
       "       'Overall Homeless, 2009', 'Overall Homeless, 2008',\n",
       "       'Overall Homeless, 2007', 'High_School_Fee', 'Elementary_School_Fee',\n",
       "       'index_x', 'mc_donalds_per_100k', 'adult_obesity_rate',\n",
       "       'eating_vegetables_daily', 'median_income', 'Vegetable',\n",
       "       'diabetes_prevalence', 'alcohol_prevalence', 'mean_physical_activity',\n",
       "       'mean_obesity', 'index_y', 'Population_x', 'Violent_Crime',\n",
       "       'Murder_and_Manslaughter', 'Rape', 'Robbery', 'Aggravated_Assoult',\n",
       "       'Property_crime', 'Burglary', 'Theft', 'Motor_Vehicle_theft',\n",
       "       'Population_y', 'Per_capita_income', 'Life_Expectancy',\n",
       "       'N_of_colleges_universities', 'N_of_junior_colleges',\n",
       "       'N_of_technical_trade_schools', 'awards_per_value', 'exp_award_value',\n",
       "       'top_230_ranking_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_list[-1]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that some columns are repeated: therefore we drop duplicate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Price_2018_Studio</th>\n",
       "      <th>Price_2018_1br</th>\n",
       "      <th>Price_2018_2br</th>\n",
       "      <th>Price_2018_3br</th>\n",
       "      <th>Price_2018_4br</th>\n",
       "      <th>Overall Homeless, 2018</th>\n",
       "      <th>Overall Homeless, 2017</th>\n",
       "      <th>Overall Homeless, 2016</th>\n",
       "      <th>Overall Homeless, 2015</th>\n",
       "      <th>...</th>\n",
       "      <th>Theft</th>\n",
       "      <th>Motor_Vehicle_theft</th>\n",
       "      <th>Per_capita_income</th>\n",
       "      <th>Life_Expectancy</th>\n",
       "      <th>N_of_colleges_universities</th>\n",
       "      <th>N_of_junior_colleges</th>\n",
       "      <th>N_of_technical_trade_schools</th>\n",
       "      <th>awards_per_value</th>\n",
       "      <th>exp_award_value</th>\n",
       "      <th>top_230_ranking_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>alabama</td>\n",
       "      <td>605.416667</td>\n",
       "      <td>659.000000</td>\n",
       "      <td>800.666667</td>\n",
       "      <td>1069.666667</td>\n",
       "      <td>1244.333333</td>\n",
       "      <td>3434</td>\n",
       "      <td>3793</td>\n",
       "      <td>4111</td>\n",
       "      <td>3970</td>\n",
       "      <td>...</td>\n",
       "      <td>2006.3</td>\n",
       "      <td>241.1</td>\n",
       "      <td>38215.0</td>\n",
       "      <td>74.813987</td>\n",
       "      <td>42.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.514493</td>\n",
       "      <td>63013.173913</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>alaska</td>\n",
       "      <td>814.333333</td>\n",
       "      <td>930.500000</td>\n",
       "      <td>1190.333333</td>\n",
       "      <td>1682.166667</td>\n",
       "      <td>2019.500000</td>\n",
       "      <td>2016</td>\n",
       "      <td>1845</td>\n",
       "      <td>1940</td>\n",
       "      <td>1956</td>\n",
       "      <td>...</td>\n",
       "      <td>2394.7</td>\n",
       "      <td>412.1</td>\n",
       "      <td>54430.0</td>\n",
       "      <td>78.915541</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.414286</td>\n",
       "      <td>141431.142857</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>arizona</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>1046.000000</td>\n",
       "      <td>1512.333333</td>\n",
       "      <td>1754.416667</td>\n",
       "      <td>9865</td>\n",
       "      <td>8947</td>\n",
       "      <td>9707</td>\n",
       "      <td>9896</td>\n",
       "      <td>...</td>\n",
       "      <td>2168.1</td>\n",
       "      <td>265.8</td>\n",
       "      <td>39955.0</td>\n",
       "      <td>78.364742</td>\n",
       "      <td>46.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>25.563492</td>\n",
       "      <td>47830.888889</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>arkansas</td>\n",
       "      <td>524.666667</td>\n",
       "      <td>571.166667</td>\n",
       "      <td>721.416667</td>\n",
       "      <td>977.583333</td>\n",
       "      <td>1144.833333</td>\n",
       "      <td>2712</td>\n",
       "      <td>2467</td>\n",
       "      <td>2463</td>\n",
       "      <td>2560</td>\n",
       "      <td>...</td>\n",
       "      <td>2233.6</td>\n",
       "      <td>239.4</td>\n",
       "      <td>39171.0</td>\n",
       "      <td>75.626900</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>21.970833</td>\n",
       "      <td>51132.479167</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>california</td>\n",
       "      <td>1212.333333</td>\n",
       "      <td>1426.333333</td>\n",
       "      <td>1818.916667</td>\n",
       "      <td>2519.250000</td>\n",
       "      <td>2926.333333</td>\n",
       "      <td>129972</td>\n",
       "      <td>131532</td>\n",
       "      <td>118142</td>\n",
       "      <td>115738</td>\n",
       "      <td>...</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>450.3</td>\n",
       "      <td>54800.0</td>\n",
       "      <td>80.231014</td>\n",
       "      <td>264.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>22.771429</td>\n",
       "      <td>63022.202857</td>\n",
       "      <td>3064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Price_2018_Studio  Price_2018_1br  Price_2018_2br  \\\n",
       "0     alabama         605.416667      659.000000      800.666667   \n",
       "1      alaska         814.333333      930.500000     1190.333333   \n",
       "2     arizona         701.000000      828.000000     1046.000000   \n",
       "3    arkansas         524.666667      571.166667      721.416667   \n",
       "4  california        1212.333333     1426.333333     1818.916667   \n",
       "\n",
       "   Price_2018_3br  Price_2018_4br  Overall Homeless, 2018  \\\n",
       "0     1069.666667     1244.333333                    3434   \n",
       "1     1682.166667     2019.500000                    2016   \n",
       "2     1512.333333     1754.416667                    9865   \n",
       "3      977.583333     1144.833333                    2712   \n",
       "4     2519.250000     2926.333333                  129972   \n",
       "\n",
       "   Overall Homeless, 2017 Overall Homeless, 2016 Overall Homeless, 2015  ...  \\\n",
       "0                    3793                   4111                   3970  ...   \n",
       "1                    1845                   1940                   1956  ...   \n",
       "2                    8947                   9707                   9896  ...   \n",
       "3                    2467                   2463                   2560  ...   \n",
       "4                  131532                 118142                 115738  ...   \n",
       "\n",
       "    Theft Motor_Vehicle_theft Per_capita_income Life_Expectancy  \\\n",
       "0  2006.3               241.1           38215.0       74.813987   \n",
       "1  2394.7               412.1           54430.0       78.915541   \n",
       "2  2168.1               265.8           39955.0       78.364742   \n",
       "3  2233.6               239.4           39171.0       75.626900   \n",
       "4  1623.0               450.3           54800.0       80.231014   \n",
       "\n",
       "  N_of_colleges_universities N_of_junior_colleges  \\\n",
       "0                       42.0                 36.0   \n",
       "1                        6.0                  1.0   \n",
       "2                       46.0                 43.0   \n",
       "3                       26.0                 26.0   \n",
       "4                      264.0                191.0   \n",
       "\n",
       "  N_of_technical_trade_schools awards_per_value  exp_award_value  \\\n",
       "0                         16.0        19.514493     63013.173913   \n",
       "1                          2.0        18.414286    141431.142857   \n",
       "2                         42.0        25.563492     47830.888889   \n",
       "3                         32.0        21.970833     51132.479167   \n",
       "4                        239.0        22.771429     63022.202857   \n",
       "\n",
       "   top_230_ranking_score  \n",
       "0                    366  \n",
       "1                     29  \n",
       "2                    209  \n",
       "3                     96  \n",
       "4                   3064  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the column Population_x into population, and drop index_x, index_y and Population_y\n",
    "df = df.rename(columns = {\n",
    "    \"Population_x\": \"Population\",\n",
    "})\n",
    "\n",
    "df = df.drop(['index_x', 'index_y', 'Population_y', 'median_income'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['State', 'Price_2018_Studio', 'Price_2018_1br', 'Price_2018_2br',\n",
       "       'Price_2018_3br', 'Price_2018_4br', 'Overall Homeless, 2018',\n",
       "       'Overall Homeless, 2017', 'Overall Homeless, 2016',\n",
       "       'Overall Homeless, 2015', 'Overall Homeless, 2014',\n",
       "       'Overall Homeless, 2013', 'Overall Homeless, 2012',\n",
       "       'Overall Homeless, 2011', 'Overall Homeless, 2010',\n",
       "       'Overall Homeless, 2009', 'Overall Homeless, 2008',\n",
       "       'Overall Homeless, 2007', 'High_School_Fee', 'Elementary_School_Fee',\n",
       "       'mc_donalds_per_100k', 'adult_obesity_rate', 'eating_vegetables_daily',\n",
       "       'Vegetable', 'diabetes_prevalence', 'alcohol_prevalence',\n",
       "       'mean_physical_activity', 'mean_obesity', 'Population', 'Violent_Crime',\n",
       "       'Murder_and_Manslaughter', 'Rape', 'Robbery', 'Aggravated_Assoult',\n",
       "       'Property_crime', 'Burglary', 'Theft', 'Motor_Vehicle_theft',\n",
       "       'Per_capita_income', 'Life_Expectancy', 'N_of_colleges_universities',\n",
       "       'N_of_junior_colleges', 'N_of_technical_trade_schools',\n",
       "       'awards_per_value', 'exp_award_value', 'top_230_ranking_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All 51 states are present in the final dataframe\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What comes next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all datasets cleaned and analyzed, we can further proceed in the realization of our project: firstly, we need to train a model which will enable us to predict the HDI index of every state of the USA, based on the dataset that we have created.\n",
    "\n",
    "This task is going to pose some challenges: in fact, our dataset has much more dimensions than samples and therefore there is going to be the risk of overfitting.\n",
    "\n",
    "We can try to counter this effect by performing some dimensionality reduction algorithms like PCA, Factor Analysis or Independent Component Analysis.\n",
    "\n",
    "As soon as we manage to reduce the dimension of our dataset, we can train the model using any regression technique like SVM or Ridge Regression.\n",
    "\n",
    "Once we compute the weights for our model, then we can build our index by adjusting the weights of our model to personalize it to the users' needs. We will offer a visual representation of it using sliders, so that the user can adjust the weights in an intuitive manner.\n",
    "\n",
    "Further, we may try to split our features into meaningful subsets (health, crime, education, monetary) in order to gauge their impact on our final HDI score. Moreover, we may try to investigate the relation among the above subsets and the actual features used to compute the UN HDI index (personal income, life expectancy and average years of education).\n",
    "\n",
    "To conclude, a further improvement could be trying to test the robustness of our model: it means that, whenever we drop any column (for instance if it happens that any of the organizations computing our metrics fails), we are still able to learn a good enough predictor for the HDI. The fact that our dataset has so many correlated features means that probably we can manage the loss of some columns, without losing much accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"Pickles/Final_dataframe.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
